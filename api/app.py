from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import StreamingResponse
import chromadb
from openai import OpenAI
from langchain.chat_models import ChatOpenAI
from langchain.schema import HumanMessage, SystemMessage
from langchain.vectorstores import Chroma
from langchain.embeddings.openai import OpenAIEmbeddings
from dotenv import load_dotenv
import os
 
load_dotenv()
 
cl√©_api = os.getenv("OPENAI_API_KEY")
chroma_db_path = os.getenv("CHROMA_DB_PATH")
 
if cl√©_api is None:
    raise ValueError("La cl√© API OpenAI n'est pas d√©finie dans le fichier .env")
 
if chroma_db_path is None:
    raise ValueError("Le chemin de la db chroma n'est pas d√©fini dans le fichier .env")
 
client = OpenAI(api_key=cl√©_api)
 
# Initialisation du mod√®le LangChain avec OpenAI
llm = ChatOpenAI(model_name="gpt-4o-mini", openai_api_key=cl√©_api)
 
# Connexion √† la base de donn√©es Chroma
chroma_client = chromadb.PersistentClient(path=chroma_db_path)
collection = chroma_client.get_collection("metier_info")
 
# Cr√©er l'application FastAPI
app = FastAPI()
 
app.add_middleware(
    CORSMiddleware,
    allow_origins=["http://localhost:5173"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)
 
# M√©morisation des conversations
user_sessions = {}
 
def retrieve_documents(query):
    results = collection.query(
        query_texts=[query],
        n_results=20,
        include=["metadatas", "documents"]
    )
   
    return results
 
# Fonction pour construire le prompt avec la conversation pr√©c√©dente
def build_prompt(query, retrieved_docs, session_id):
    context = retrieved_docs
   
    # R√©cup√©rer l'historique de conversation
    history = user_sessions.get(session_id, [])
    formatted_history = "\n".join([f"{msg['role']}: {msg['content']}" for msg in history])
   
    prompt = f"""
    Voici des informations pertinentes :
    {context}
   
    Historique de la conversation :
    {formatted_history}
   
    Question utilisateur : {query}
    R√©ponds de mani√®re d√©taill√©e et pr√©cise.
    """
   
    return prompt
 
# Fonction pour construire le prompt avec la conversation pr√©c√©dente
def build_prompt_motivation(query, session_id):
   
    # R√©cup√©rer l'historique de conversation
    history = user_sessions.get(session_id, [])
    formatted_history = "\n".join([f"{msg['role']}: {msg['content']}" for msg in history])
   
    prompt = f"""    
    Historique de la conversation :
    {formatted_history}
   
    Question utilisateur : {query}
    R√©ponds de mani√®re d√©taill√©e et pr√©cise.
    """
   
    return prompt
# Route API pour traiter la requ√™te utilisateur et g√©n√©rer la r√©ponse via query parameter
@app.get("/generate")
async def generate_response_api(query: str, session_id: str):
    try:
        if not session_id:
            raise HTTPException(status_code=400, detail="Le param√®tre 'session_id' est obligatoire")
 
        retrieved_docs = retrieve_documents(query)
        prompt = build_prompt(query, retrieved_docs, session_id)
 
        # Affichage du prompt dans les logs du serveur
        print("\n===== Prompt envoy√© au LLM =====\n")
        print(prompt)
        print("\n================================\n")
 
        return StreamingResponse(generate_response(prompt, session_id), media_type="text/plain")
 
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))
 
# Fonction pour g√©n√©rer une r√©ponse via OpenAI
def generate_response(prompt, session_id):
    stream = client.chat.completions.create(
        model="gpt-4o-mini",
        messages = [
            {
                "role": "system",
                "content": """
                   Tu es un assistant d'orientation professionnelle expert. Ta mission est d'aider l'utilisateur √† explorer les m√©tiers et les formations qui lui correspondent en engageant un dialogue interactif et progressif.
                    L'utilisateur pourra √©galement te copier/coller des offres d'emploi et/ou de formations et tu devras le guider sur les attendus et l'aider √† comprendre ces textes.
                    Principes cl√©s:
                    - Engagement progressif : Ne pas inonder d‚Äôinformations. Poser des questions pour affiner la discussion au fur et √† mesure.
                    - √âchange naturel: Utiliser un ton conversationnel, √©viter les r√©ponses encyclop√©diques.
                    - Adaptation √† l‚Äôutilisateur : Relancer avec des questions personnalis√©es selon ses r√©ponses.
                    - Ne pas inclure "assistant :" en d√©but de r√©ponse. R√©ponds directement dans un format conversationnel naturel.
                   
                    ### üõ† **Exemples d'√©changes** (Few-Shot) :
                    **Utilisateur :** Je ne sais pas quel m√©tier choisir.  
                    **Assistant :** Pas de souci ! üòä Peux-tu me dire ce que tu aimes faire au quotidien ?  
                    **Utilisateur :** J‚Äôaime travailler avec les chiffres.  
                    **Assistant :** Int√©ressant ! üìä Est-ce que tu pr√©f√®res les analyser, faire des pr√©visions ou les organiser ?  
                    **Utilisateur :** J‚Äôaime surtout les analyser.  
                    **Assistant :** Super ! üéØ Tu pourrais aimer des m√©tiers comme **data analyst**, **actuaire** ou **contr√¥leur de gestion**. Veux-tu plus d‚Äôinfos sur l‚Äôun d‚Äôeux ?  
                    **Utilisateur :** Oui, sur le data analyst.  
                    **Assistant :** Bien s√ªr ! Voici un r√©sum√© clair üëá  
                   
markdown
                    ### M√©tier : Data Analyst
                    **Description** : Sp√©cialiste de l‚Äôanalyse des donn√©es, il aide les entreprises √† prendre des d√©cisions strat√©giques en traitant et visualisant des informations.  
                    **Salaire moyen** : Environ 40 000‚Ç¨ par an.  
                    **Comp√©tences requises** : Statistiques, programmation (Python, SQL), esprit analytique.  
                    **Formation** : Master en data science, statistiques ou √©conomie.  
                    **üí° Et apr√®s ?**  
                    - Est-ce que cela te parle ou veux-tu explorer d'autres m√©tiers li√©s aux chiffres ?
                    - Pr√©f√®res-tu un m√©tier plus orient√© terrain ou strat√©gie ?
 
                    Ta mission :  
                    - Toujours engager la conversation avec des relances.
                    - Fournir des infos pertinentes en gardant un ton fluide et interactif.
                    - Ne jamais r√©pondre avec un long pav√© sans relancer l‚Äô√©change.
                    Si l‚Äôutilisateur est ind√©cis, l‚Äôaider √† affiner en posant une question √† la fois.  
                    Style de r√©ponse :
                    - √âvite de commencer chaque r√©ponse par "C'est super !" ou "G√©nial !".  
                    - Adapte ton ton en fonction du contexte :  
                    - Si l'utilisateur partage une h√©sitation ‚Üí Montre de l‚Äôempathie ("Je vois, ce n‚Äôest pas facile de choisir‚Ä¶")  
                    - S‚Äôil exprime une envie ou une id√©e ‚Üí Encourage sans exag√©rer ("C'est une bonne piste, tu veux qu'on explore ensemble les d√©bouch√©s ?")  
                    - S‚Äôil pose une question technique ‚Üí Reste neutre et informatif.  
                    - Ne r√©p√®te pas les m√™mes formules √† chaque r√©ponse. Varie ton langage pour garder la conversation naturelle.
                   
                   
 
                """
            },
            {
                "role": "user",
                "content": prompt
            }
        ],
        stream=True,
    )
 
    full_response = ""
   
    for chunk in stream:
        if chunk.choices[0].delta.content is not None:
            yield chunk.choices[0].delta.content
            full_response += chunk.choices[0].delta.content
 
    # M√©moriser la conversation
    user_sessions.setdefault(session_id, []).append({"role": "user", "content": prompt})
    user_sessions[session_id].append({"role": "assistant", "content": full_response})
 
 
# Fonction pour g√©n√©rer une r√©ponse via OpenAI
def generate_response_motivation(prompt, session_id):
    stream = client.chat.completions.create(
        model="gpt-4o-mini",
        messages = [
            {
                "role": "system",
                "content": """
                    Tu es un assistant expert en r√©daction de lettres de motivation. Ta mission est d'aider l'utilisateur √† r√©diger, corriger et optimiser sa lettre pour maximiser ses chances d'obtenir un entretien. Tu peux √©galement analyser les fiches de poste qu'il fournit.
 
                    ### R√®gles de r√©ponse :
                    - **Approche personnalis√©e :** Pose des questions pour adapter la lettre au profil du candidat et au poste vis√©.
                    - **Structure claire en markdown :**
                      - üìå **Introduction :** Pr√©sente-toi bri√®vement et engage l'utilisateur avec une phrase encourageante.
                      - üîç **Corrections et suggestions :** Analyse la lettre et propose des am√©liorations (phrases plus impactantes, reformulations, ajustements).
                      - üéØ **Conseils personnalis√©s :** Donne des astuces sur le ton, la mise en page et l‚Äôimpact des formulations.
                      - ‚úÖ **Version finale optimis√©e :** R√©√©cris la lettre en tenant compte des ajustements.
 
                    ### Format de r√©ponse initial :
                    üìå **Questions pr√©liminaires qui doivent √™tre pos√©es une par une et non pas en une fois :**
                    1. Quel est le poste que tu vises ou peux-tu me fournir la fiche de poste ?
                    2. Quelles √©tudes as-tu suivies et quels sont tes points forts ?
                    3. Peux-tu d√©crire tes exp√©riences professionnelles pass√©es pertinentes pour ce poste ?
                   
                    Une fois que l'utilisateur a fourni ces informations, tu g√©n√©reras une lettre de motivation personnalis√©e et demanderas si cela lui convient ou si des modifications sont n√©cessaires.
                   
                    **Message d‚Äôaccueil (premi√®re interaction seulement) :**  
                    "Bienvenue ! Ensemble, cr√©ons une lettre de motivation percutante. Dis-moi pour quel poste tu postules et quelles sont tes exp√©riences cl√©s ! üöÄ"
                """
            },
            {
                "role": "user",
                "content": prompt
            }
        ],
        stream=True,
    )
 
    full_response = ""
   
    for chunk in stream:
        if chunk.choices[0].delta.content is not None:
            yield chunk.choices[0].delta.content
            full_response += chunk.choices[0].delta.content
 
    # M√©moriser la conversation
    user_sessions.setdefault(session_id, []).append({"role": "user", "content": prompt})
    user_sessions[session_id].append({"role": "assistant", "content": full_response})
 
 
# Route API pour traiter la requ√™te utilisateur et g√©n√©rer la r√©ponse via query parameter
@app.get("/generate")
async def generate_response_api(query: str, session_id: str):
    try:
        # V√©rifier si session_id est fourni
        if not session_id:
            raise HTTPException(status_code=400, detail="Le param√®tre 'session_id' est obligatoire")
 
        # R√©cup√©rer les documents en fonction de la requ√™te
        retrieved_docs = retrieve_documents(query)
 
        # Construire le prompt
        prompt = build_prompt(query, retrieved_docs, session_id)
 
        # Stream the response using StreamingResponse
        return StreamingResponse(generate_response(prompt, session_id), media_type="text/plain")
 
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))
 
# Route API pour traiter la requ√™te utilisateur et g√©n√©rer la r√©ponse via query parameter
@app.get("/generate_motivation")
async def generate_response_api(query: str, session_id: str):
    try:
        # V√©rifier si session_id est fourni
        if not session_id:
            raise HTTPException(status_code=400, detail="Le param√®tre 'session_id' est obligatoire")
 
        # Construire le prompt
        prompt = build_prompt_motivation(query, session_id)
 
        # Stream the response using StreamingResponse
        return StreamingResponse(generate_response_motivation(prompt, session_id), media_type="text/plain")
 
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))